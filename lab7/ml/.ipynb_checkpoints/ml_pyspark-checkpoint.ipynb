{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27e7f5c5-1f5e-4773-8950-66be75241122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, FloatType, LongType, StringType, DoubleType\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, Imputer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import pyspark.sql.functions as F\n",
    "from itertools import combinations\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f0ac94-ae99-42c8-8528-3b62b4e191ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4a5270-37a8-4a0e-9663-2f0c09b88269",
   "metadata": {},
   "source": [
    "## Check Python Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a06b828d-1d7a-41a3-8929-703028401ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/demos/bin/python3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e12fef7f-59b8-4b2b-b7b6-f2fff328616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data\"\n",
    "\n",
    "NUMBER_OF_FOLDS = 3\n",
    "SPLIT_SEED = 7576\n",
    "TRAIN_TEST_SPLIT = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a794b4a-3efa-48bc-a8c3-2df602c90670",
   "metadata": {},
   "source": [
    "## Function for data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99ab6c2d-df8f-4aaf-bdfe-a6f30efaba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_data(spark: SparkSession) -> DataFrame:\n",
    "    \"\"\"\n",
    "    read data; since the data has the header we let spark guess the schema\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the Titanic CSV data into a DataFrame\n",
    "    titanic_data = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(os.path.join(DATA_FOLDER,\"heart_disease.csv\"))\n",
    "\n",
    "    return titanic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee6afbb-f7ce-4f7a-897b-070bfb496efe",
   "metadata": {},
   "source": [
    "## Writing new Transformer type class : adding cross product of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cef9e877-a67b-4757-96d6-571c8fc02579",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairwiseProduct(Transformer):\n",
    "\n",
    "    def __init__(self, inputCols, outputCols):\n",
    "        self.__inputCols = inputCols\n",
    "        self.__outputCols = outputCols\n",
    "\n",
    "        self._paramMap = self._params = {}\n",
    "\n",
    "    def _transform(self, df):\n",
    "        for cols, out_col in zip(self.__inputCols, self.__outputCols):\n",
    "            df = df.withColumn(out_col, col(cols[0]) * col(cols[1]))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da37d86-c3aa-43c2-a838-7b072140259e",
   "metadata": {},
   "source": [
    "## The ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1d53e22-eb2c-4e85-bf7f-12ec742721a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def pipeline(data: DataFrame):\n",
    "\n",
    "    \"\"\"\n",
    "    every attribute that is numeric is non-categorical; this is questionable\n",
    "    \"\"\"\n",
    "\n",
    "    numeric_features = [f.name for f in data.schema.fields if isinstance(f.dataType, (DoubleType, FloatType, IntegerType, LongType))]\n",
    "    string_features = [f.name for f in data.schema.fields if isinstance(f.dataType, StringType)]\n",
    "    #numeric_features.remove(\"PassengerId\")\n",
    "    #numeric_features.remove(\"Survived\")\n",
    "    #string_features.remove(\"Name\")\n",
    "    print(numeric_features)\n",
    "    print(string_features)\n",
    "\n",
    "    # Fill missing values for string columns with a placeholder before indexing\n",
    "    data = data.fillna({col: 'null' for col in string_features})\n",
    "    \n",
    "    # Index string features\n",
    "    indexed_string_columns = [f\"{v}Index\" for v in string_features]\n",
    "    indexers = [StringIndexer(inputCol=col, outputCol=indexed_col, handleInvalid='keep') for col, indexed_col in zip(string_features, indexed_string_columns)]\n",
    "\n",
    "    # Impute missing values for indexed string columns\n",
    "    imputed_columns_string = [f\"Imputed{v}\" for v in indexed_string_columns]\n",
    "    imputer_string = Imputer(inputCols=indexed_string_columns, outputCols=imputed_columns_string, strategy=\"mode\")\n",
    "\n",
    "    \n",
    "    # numeric columns\n",
    "    imputed_columns_numeric = [f\"Imputed{v}\" for v in numeric_features]\n",
    "    imputer_numeric = Imputer(inputCols=numeric_features, outputCols=imputed_columns_numeric, strategy = \"mean\")\n",
    "\n",
    "    # Create all pairwise products of numeric features\n",
    "    #all_pairs = [v for v in combinations(imputed_columns_numeric, 2)]\n",
    "    #pairwise_columns = [f\"{col1}_{col2}\" for col1, col2 in all_pairs]\n",
    "    #pairwise_product = PairwiseProduct(inputCols=all_pairs, outputCols=pairwise_columns)\n",
    "\n",
    "    # Assemble feature columns into a single feature vector\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=imputed_columns_numeric + imputed_columns_string, \n",
    "        outputCol=\"features\"\n",
    "        )\n",
    "\n",
    "    # Create a list of pipeline stages\n",
    "    stages = indexers + [imputer_string, imputer_numeric, assembler]\n",
    "    \n",
    "    # Create and fit the pipeline\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    model = pipeline.fit(data)\n",
    "    \n",
    "    # Transform the data\n",
    "    transformed_data = model.transform(data)\n",
    "    \n",
    "    return transformed_data\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00f200e4-2378-423b-98be-f9996c4a699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Create a Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Predict Titanic Survival\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    try:\n",
    "        # Read data\n",
    "        data = read_data(spark)\n",
    "        \n",
    "        # Print schema and preview the data\n",
    "        data.printSchema()\n",
    "        data.show(5)\n",
    "\n",
    "        # Apply the pipeline\n",
    "        transformed_data = pipeline(data)\n",
    "        \n",
    "        # Show the transformed data, including the imputed columns\n",
    "        columns_to_show = [col for col in transformed_data.columns if col.startswith(\"Imputed\")]\n",
    "        transformed_data.select(columns_to_show).show(truncate=False)\n",
    "        \n",
    "    finally:\n",
    "        # Stop the Spark session\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315b3402-ad5f-4e46-a317-e0c3d804963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6ba500-308a-454f-be09-03e495ad9a49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
