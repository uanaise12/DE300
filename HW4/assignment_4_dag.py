# -*- coding: utf-8 -*-
"""assignment_4_dag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hHHkhY7pex6R4uhL9js7SwMn_yWPIgss
"""

#Import libraries
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime
from airflow.operators.dummy_operator import DummyOperator
import pandas as pd
import boto3
from io import BytesIO
import requests
from bs4 import BeautifulSoup
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.impute import SimpleImputer
import numpy as np

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 6, 1),
    'retries': 1,
}

dag = DAG('assignment_4_dag', default_args=default_args, schedule_interval='@daily')

start = DummyOperator(task_id='start', dag=dag)

# Function from hw2 (Sklearn)
def load_data():
  s3 = boto3.client('s3',
                aws_access_key_id='',
                aws_secret_access_key='',
                aws_session_token='')


  bucket_name = 'de300spring2024'
  object_key = 'anaise/hw2/heart_disease(in).csv'
  csv_obj = s3.get_object(Bucket=bucket_name, Key=object_key)
  body = csv_obj['Body']
  csv_string = body.read().decode('utf-8')
  df1 = pd.read_csv(BytesIO(csv_string.encode()))
  columns_to_keep = [
    'age', 'sex', 'painloc', 'painexer', 'cp', 'trestbps', 'smoke', 'fbs',
    'prop', 'nitr', 'pro', 'diuretic', 'thaldur', 'thalach', 'exang',
    'oldpeak', 'slope', 'target'
  ]

  # Select only the specified columns
  df = df1[columns_to_keep]
  #save a copy of the dataframe we want to use
  df.to_csv('data.csv', index=False)

#EDA using sklearn from HW2
def standard_feature_engineering():
  df = pd.read_csv('data.csv')


  """Removing rows whose value of target is null first"""

  df.dropna(subset=['target'], inplace=True)

  """Imputing trestbps using median imputation since it's a numerical value"""

  # Calculate the median of the values greater than or equal to 100 mm Hg
  replacement_value = df['trestbps'][df['trestbps'] >= 100].median()

  # Replace values less than 100 mm Hg or null values with the median
  df['trestbps'] = df['trestbps'].apply(lambda x: replacement_value if pd.isnull(x) or x < 100 else x)

  # Verify the changes
  print(df['trestbps'])

  """*Imputing* oldpeak, excluding the thresholds"""

  # Calculate the median of the values within the valid range (0 to 4)
  replacement_value = df['oldpeak'][(df['oldpeak'] >= 0) & (df['oldpeak'] <= 4)].median()

  # Replace values less than 0, greater than 4, or null with the median
  df['oldpeak'] = df['oldpeak'].apply(lambda x: replacement_value if pd.isnull(x) or x < 0 or x > 4 else x)

  # Verify the changes
  print(df['oldpeak'])

  """Imputing thaldur and thalach using median imputation (replacing in the null values)"""

  # Calculate the median for 'thaldur'
  thaldur_replacement = df['thaldur'].median()

  # Calculate the median for 'thalach'
  thalach_replacement = df['thalach'].median()

  # Replace missing values in 'thaldur' with the median
  df['thaldur'].fillna(thaldur_replacement, inplace=True)

  # Replace missing values in 'thalach' with the median
  df['thalach'].fillna(thalach_replacement, inplace=True)
  # Verify changes by checking for any remaining missing values
  print(df[['thaldur', 'thalach']].isnull().sum())

  """Imputing fbs, prop, nitr, pro, diuretic by
  replacing the missing values and values greater than 1.
  """

  # List of specified columns
  columns = ['fbs', 'prop', 'nitr', 'pro', 'diuretic']

  # Loop through each column to replace values and handle missing data
  for col in columns:
      # Replace values greater than 1 with 1
      df[col] = df[col].apply(lambda x: 1 if x > 1 else x)

      # Calculate the mode (most common value in the column)
      mode_value = df[col].mode().iloc[0]

      # Replace missing values with the mode
      df[col].fillna(mode_value, inplace=True)
  # Check unique values and missing values count
  for col in columns:
      print(f"Unique values in {col}: {df[col].unique()}")
      print(f"Missing values in {col}: {df[col].isnull().sum()}")

  """Imputing exang and slope by replacing the missing values with the mode."""

  # Calculate the mode for each column
  exang_mode = df['exang'].mode()[0]
  slope_mode = df['slope'].mode()[0]
  # Replace missing values with the mode
  df['exang'] = df['exang'].fillna(exang_mode)
  df['slope'] = df['slope'].fillna(slope_mode)
  # Verify changes by checking for any remaining missing values
  print(df[['exang', 'slope']].isnull().sum())
  # Optional: Check the distributions to see how the mode has impacted the data
  print(df['exang'])
  print(df['slope'].value_counts())

  """Cleaning out the age column so that it can be used for finding out the corresponding age range.

  We will use median imputation
  """

  # Convert age to numeric, coercing errors and setting invalid entries to NaN
  df['age'] = pd.to_numeric(df['age'], errors='coerce')

  # Calculate the median age from valid entries (ignores NaN by default)
  median_age = df['age'].median()

  # Replace NaN values in the 'age' column with the median age
  df['age'].fillna(median_age, inplace=True)

"""Scraping for the smoke column

1. Using https://www.abs.gov.au/statistics/health/health-conditions-and-risks/smoking/latest-release
"""

# URL of the webpage
url = 'https://www.abs.gov.au/statistics/health/health-conditions-and-risks/smoking/latest-release'

# Send GET request
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

# Find all tables with a specific caption
tables = soup.find_all('table', class_='responsive-enabled')
table = None
for tbl in tables:
    caption = tbl.find('caption')
    if caption and 'Proportion of people 15 years and over who were current daily smokers by age, 2011–12 to 2022' in caption.text:
        table = tbl
        break

# Check if the correct table is found
if not table:
    print("The specified table was not found.")
else:
    # Initialize a list to store each row's data
    data = []

    # Iterate over each row in the table body
    for row in table.find_all('tr')[1:]:  # Skip the header row
        cols = row.find_all('td')
        age_group = row.th.text.strip() if row.th else 'No Age Group'  # Ensuring to get the age group if available
        row_data = [age_group] + [col.text.strip() for col in cols]
        data.append(row_data)

    # Define column headers based on the table's structure
    columns = ['Age Group', '2011-12 (%)', 'CI Low (2011-12)', 'CI High (2011-12)',
               '2014-15 (%)', 'CI Low (2014-15)', 'CI High (2014-15)',
               '2017-18 (%)', 'CI Low (2017-18)', 'CI High (2017-18)',
               '2022 (%)', 'CI Low (2022)', 'CI High (2022)']

    # Create a DataFrame
    smoking_data_df = pd.DataFrame(data, columns=columns)

    # Display the first few rows of the DataFrame to check
    print(smoking_data_df['Age Group'],smoking_data_df['2022 (%)'])
    print (type(smoking_data_df['Age Group'][1]))
    print (type(smoking_data_df['2022 (%)'][1]))
    print(df['smoke'])

#create a new column for first scraping:

def get_smoking_rate(row):
    if pd.isna(row['smoke']):  # Check if 'smoke' is NaN for the current row
        age = int(row['age'])  # Convert age to integer
        for index, age_data in smoking_data_df.iterrows():
            age_range = age_data['Age Group']
            if age_range == '75 years and over':
                if age >= 75:
                    return age_data['2022 (%)']
            else:
                low, high = map(int, age_range.split('–'))
                if low <= age <= high:
                    return age_data['2022 (%)']
        return None  # Return None if no appropriate age group is found
    else:
        return row['smoke']  # Return the existing value if it's not NaN

# Apply the function to create the 'smoke_abs' column, which is meant to be an updated 'smoke' column
df['smoke_abs'] = df.apply(get_smoking_rate, axis=1)

# Print the modified DataFrame to verify the changes
print(df[['age', 'smoke', 'smoke_abs']])

"""[link text](https://)Imputing the sex column using mode imputation"""

# Calculate the mode of the 'sex' column
mode_sex = df['sex'].mode()[0]

# Replace missing values in the 'sex' column with the mode
df['sex'].fillna(mode_sex, inplace=True)

"""Scraping CDC data by sex"""

# URL of the webpage
url = 'https://www.cdc.gov/tobacco/data_statistics/fact_sheets/adult_data/cig_smoking/index.htm'

# Send GET request
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

# Initialize a dictionary to store the gender-specific smoking rates
smoking_rates_by_gender_cdc = {}

# Find all card-body containers
cards = soup.find_all('div', class_='card-body')

# Look for the card that contains the phrase "Current cigarette smoking was higher among men than women"
gender_card = None
for card in cards:
    if "Current cigarette smoking was higher among men than women" in card.text:
        gender_card = card
        break

# Check if the specific card is found and then process the list items
if gender_card:
    list_items = gender_card.find_all('li')
    for item in list_items:
      text = item.get_text().strip()
        # Extract gender and percentage
        gender = 'men' if 'adult men' in text else 'women'
        percentage = text.split('(')[-1].rstrip('%)').strip()
        smoking_rates_by_gender_cdc[gender] = float(percentage)
else:
    print("Gender-specific smoking section not found.")

# Output the dictionary to verify the extracted data
print(smoking_rates_by_gender_cdc)

"""Scraping CDC data by age"""

# URL of the webpage
url = 'https://www.cdc.gov/tobacco/data_statistics/fact_sheets/adult_data/cig_smoking/index.htm'

# Send GET request
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

# Initialize a dictionary to store the age-specific smoking rates
smoking_rates_by_age_cdc = {}

# Find all card-body containers
cards = soup.find_all('div', class_='card-body')

# Look for the card that contains the phrase "Current cigarette smoking was highest among"
age_card = None
for card in cards:
    if "Current cigarette smoking was highest among" in card.text:
        age_card = card
        break

# Check if the specific card is found and then process the list items
if age_card:
    list_items = age_card.find_all('li')
    for item in list_items:
        text = item.get_text().strip()
        # Extract age range and percentage
        age_range = text.split('adults aged')[1].split('(')[0].strip()
        percentage = text.split('(')[-1].rstrip('%)').strip()
        smoking_rates_by_age_cdc[age_range] = float(percentage)
else:
    print("Age-specific smoking section not found.")

# Output the dictionary to verify the extracted data
print(smoking_rates_by_age_cdc)

def get_smoking_rate_cdc(row):
    if pd.isna(row['smoke']):  # Check if 'smoke' is NaN
    # Assign age group based on age
        if 18 <= row['age'] < 25:
            age_group = '18–24 years'
        elif 25 <= row['age'] < 45:
            age_group = '25–44 years'
        elif 45 <= row['age'] < 65:
            age_group = '45–64 years'
        elif row['age'] >= 65:
            age_group = '65 years and older'
        else:
            return None  # Return None for ages outside defined ranges or invalid entries

        # Get the base rate for the age group
        base_rate = float(smoking_rates_by_age_cdc.get(age_group, 0))

        if pd.isna(row['sex']):
            return None  # If gender is NaN, return None

        # Calculate smoking rate based on gender
        if row['sex'] == 1:  # Male
            gender_factor = smoking_rates_by_gender_cdc['men'] / smoking_rates_by_gender_cdc['women']
            return base_rate * gender_factor
        elif row['sex'] == 0:  # Female
            return base_rate
    else:
        return row['smoke']  # Return existing 'smoke' value if not NaN

# Apply the function to update the 'smoke' column only for missing values
df['smoke_cdc'] = df.apply(get_smoking_rate_cdc, axis=1)

# Print the modified DataFrame to verify the changes
print(df[['age', 'sex', 'smoke', 'smoke_cdc']])

"""Imputing painloc and painexer columns using mode imputation because they have categorical values"""

painloc_mode = df['painloc'].mode()[0]
# Replace missing values with the mode
df['painloc'] = df['painloc'].fillna(painloc_mode)

painexer_mode = df['painexer'].mode()[0]
# Replace missing values with the mode
df['painexer'] = df['painexer'].fillna(painexer_mode)

"""Applying transformations to smoke columns"""

# Convert string to float
df['smoke_abs'] = pd.to_numeric(df['smoke_abs'], errors='coerce')
df['smoke_cdc'] = pd.to_numeric(df['smoke_cdc'], errors='coerce')

# Applying log transformation and handling zero or negative values by adding a small constant
# Convert percentages to a proportion, then apply log transform
df['smoke_abs'] = df['smoke_abs'].apply(lambda x: np.log(x / 100 + 10) if not pd.isna(x) else x)
df['smoke_cdc'] = df['smoke_cdc'].apply(lambda x: np.log(x / 100 + 10) if not pd.isna(x) else x)
# Check the output to ensure the transformation was applied correctly
print(df[['smoke_abs', 'smoke_cdc']].describe())

#Remove the original smoke column, to eliminate the null values for model training.
df = df.drop(columns=['smoke'])
#Save the cleaned data
df.to_csv('cleaned_data1.csv', index=False)

# Function from hw3 (Spark)
def spark_eda():
  #import data
  df = pd.read_csv('data.csv')

  from pyspark.sql import SparkSession
  from pyspark.sql.functions import col, when

  spark = SparkSession.builder.master("local[*]").appName("HeartDiseaseAnalysis").getOrCreate()

  #Importing libraries
  from pyspark.ml import Pipeline
  from pyspark.ml.feature import VectorAssembler
  from pyspark.ml.classification import RandomForestClassifier
  from pyspark.ml.evaluation import MulticlassClassificationEvaluator
  from pyspark.ml.classification import RandomForestClassifier, GBTClassifier
  from pyspark.ml.feature import Imputer
  from pyspark.sql.functions import when, count, broadcast
  from pyspark.sql.functions import col, expr, lit,udf
  from pyspark.sql.window import Window
  from pyspark.sql import functions as F
  from pyspark.sql.types import DoubleType, FloatType
  from pyspark.sql.typ

  """Removing rows whose value of target is null first"""

  # Drop rows where the 'target' column has missing values
  df = df.na.drop(subset=["target"])

  """Imputing trestbps using median imputation since it's a numerical value"""

  # Filter the DataFrame to include only rows where 'trestbps' is greater than or equal to 100
  # Then calculate the median of these filtered values
  median_df = df.filter(col('trestbps') >= 100)
  replacement_value = median_df.approxQuantile('trestbps', [0.5], 0.01)[0]  # 0.01 is the relative error, adjust as needed
  # Replace 'trestbps' values less than 100 or null with the median, otherwise keep original
  df = df.withColumn('trestbps', when((col('trestbps') < 100) | col('trestbps').isNull(), replacement_value).otherwise(col('trestbps')))
  df.select('trestbps').show()

  """Imputing oldpeak, excluding the thresholds"""

  # Filter the DataFrame to include only rows where 'oldpeak' is between 0 and 4
  valid_range_df = df.filter((col('oldpeak') >= 0) & (col('oldpeak') <= 4))

  # Calculate the median of these filtered values
  replacement_value = valid_range_df.approxQuantile('oldpeak', [0.5], 0.01)[0]
  # Replace 'oldpeak' values outside the range 0-4 or null with the median, otherwise keep original
  df = df.withColumn('oldpeak', when((col('oldpeak') < 0) | (col('oldpeak') > 4) | col('oldpeak').isNull(), replacement_value).otherwise(col('oldpeak')))
  # Show the updated column to verify the replacement
  df.select('oldpeak').show()

  """Imputing thaldur and thalach using median imputation (replacing in the null values)"""

  # Calculate the median for 'thaldur'
  thaldur_median = df.approxQuantile('thaldur', [0.5], 0.01)[0]

  # Calculate the median for 'thalach'
  thalach_median = df.approxQuantile('thalach', [0.5], 0.01)[0]
  # Replace missing values in 'thaldur' with the median
  df = df.na.fill({'thaldur': thaldur_median})

  # Replace missing values in 'thalach' with the median
  df = df.na.fill({'thalach': thalach_median})

  """Imputing fbs, prop, nitr, pro, diuretic by
  replacing the missing values and values greater than 1.
  """

  def replace_values(df, column):
      """ Replace values greater than 1 with 1 in the specified column """
      return df.withColumn(column, when(col(column) > 1, 1).otherwise(col(column)))

  def calculate_mode(df, column):
      """ Calculate the mode (most frequent value) of the specified column """
      mode_value = df.groupBy(column).count().orderBy('count', ascending=False).first()[0]
      return mode_value

  columns = ['fbs', 'prop', 'nitr', 'pro', 'diuretic']
  for column in columns:
      # Replace values greater than 1 with 1
      df = replace_values(df, column)

      # Calculate the mode of the column
      mode_value = calculate_mode(df, column)

      # Replace missing values with the mode
      df = df.na.fill({column: mode_value})

  """Imputing exang and slope by replacing the missing values with the mode."""

  def calculate_mode(df, column_name):
      """ Calculate the mode (most common value) for the specified column """
      # Group by the column and count occurrences, then order by count descending and take the first
      mode_value = df.groupBy(column_name).count().orderBy(col("count").desc()).first()[0]
      return mode_value

  # Calculate mode for 'exang'
  exang_mode = calculate_mode(df, 'exang')

  # Calculate mode for 'slope'
  slope_mode = calculate_mode(df, 'slope')

  # Replace missing values with the mode
  from pyspark.sql.functions import when
  df = df.withColumn("exang", when(col("exang").isNull(), exang_mode).otherwise(col("exang")))
  df = df.withColumn("slope", when(col("slope").isNull(), slope_mode).otherwise(col("slope")))

  """Cleaning out the age column so that it can be used for finding out the corresponding age range.

  We will use median imputation
  """

  # Attempt to cast the 'age' column to double type (float in Spark SQL)
  # Invalid entries will be turned into nulls
  df = df.withColumn("age", col("age").cast(DoubleType()))
  # Calculate the median age, ignoring nulls by default
  median_age = df.approxQuantile("age", [0.5], 0.01)[0]
  # Fill NaN values in the 'age' column with the median age
  df = df.na.fill({"age": median_age})

  """Scraping for the smoke column

Using https://www.abs.gov.au/statistics/health/health-conditions-and-risks/smoking/latest-release
"""

# URL of the webpage
url = 'https://www.abs.gov.au/statistics/health/health-conditions-and-risks/smoking/latest-release'

# Send GET request
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

# Find all tables with a specific caption
tables = soup.find_all('table', class_='responsive-enabled')
table = None
for tbl in tables:
    caption = tbl.find('caption')
    if caption and 'Proportion of people 15 years and over who were current daily smokers by age, 2011–12 to 2022' in caption.text:
        table = tbl
        break

# Check if the correct table is found
if not table:
    print("The specified table was not found.")
else:
    # Initialize a list to store each row's data
    data = []

    # Iterate over each row in the table body
    for row in table.find_all('tr')[1:]:  # Skip the header row
        cols = row.find_all('td')
        age_group = row.th.text.strip() if row.th else 'No Age Group'  # Ensuring to get the age group if available
        row_data = [age_group] + [col.text.strip() for col in cols]
        data.append(row_data)

    # Define column headers based on the table's structure
    columns = ['Age Group', '2011-12 (%)', 'CI Low (2011-12)', 'CI High (2011-12)',
               '2014-15 (%)', 'CI Low (2014-15)', 'CI High (2014-15)',
               '2017-18 (%)', 'CI Low (2017-18)', 'CI High (2017-18)',
               '2022 (%)', 'CI Low (2022)', 'CI High (2022)']

    # Create a DataFrame
    smoking_data_df = pd.DataFrame(data, columns=columns)

# Convert pandas DataFrame to Spark DataFrame if not already done
smoking_data_spark_df = spark.createDataFrame(smoking_data_df)

# Define a UDF to replace NaN in 'smoke' based on age groups
@udf(StringType())  # Specify the return type of the UDF
def get_smoking_rate(age, smoke):
    age_groups = broadcast_age_groups.value
    if smoke is None:
        age = int(age)
        for age_range, rate in age_groups.items():
            if age_range == '75 years and over':
                if age >= 75:
                    return rate
            else:
                low, high = map(int, age_range.split('–'))
                if low <= age <= high:
                    return rate
        return None
    else:
        return smoke

# Collect age groups and rates into a dictionary for broadcast to workers
age_groups_rates = {row['Age Group']: row['2022 (%)'] for row in smoking_data_spark_df.collect()}
broadcast_age_groups = spark.sparkContext.broadcast(age_groups_rates)

# Apply the UDF to the DataFrame
df = df.withColumn("smoke_abs", get_smoking_rate(col("age"), col("smoke")))

# Show the result
df.select("age", "smoke", "smoke_abs").show()

"""Imputing the sex column using mode imputation"""

# Calculate the mode of the 'sex' column
mode_sex_df = df.groupBy("sex").count().orderBy(col("count").desc()).limit(1)
mode_sex = mode_sex_df.collect()[0]['sex']
# Replace missing values in the 'sex' column with the mode
df = df.na.fill({'sex': mode_sex})

"""Scraping CDC data by sex"""

# URL of the webpage
url = 'https://www.cdc.gov/tobacco/data_statistics/fact_sheets/adult_data/cig_smoking/index.htm'

# Send GET request
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

# Initialize a dictionary to store the gender-specific smoking rates
smoking_rates_by_gender_cdc = {}

# Find the relevant card containing smoking data by gender
cards = soup.find_all('div', class_='card-body')
gender_card = next((card for card in cards if "Current cigarette smoking was higher among men than women" in card.text), None)

# Process the relevant data if found
if gender_card:
    list_items = gender_card.find_all('li')
    for item in list_items:
        text = item.get_text().strip()
        gender = 'men' if 'adult men' in text else 'women'
        percentage = text.split('(')[-1].rstrip('%)').strip()
        smoking_rates_by_gender_cdc[gender] = float(percentage)
else:
    print("Gender-specific smoking section not found.")

# Output the dictionary to verify the extracted data
print(smoking_rates_by_gender_cdc)

def scrape_smoking_rates(url, search_text):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    smoking_rates = {}

    cards = soup.find_all('div', class_='card-body')
    age_card = next((card for card in cards if search_text in card.text), None)

    if age_card:
        list_items = age_card.find_all('li')
        for item in list_items:
            text = item.get_text().strip()
            age_range = text.split('adults aged')[1].split('(')[0].strip()
            percentage = text.split('(')[-1].rstrip('%)').strip()
            smoking_rates[age_range] = float(percentage)
    else:
        print(f"{search_text} section not found.")

    return smoking_rates

url = 'https://www.cdc.gov/tobacco/data_statistics/fact_sheets/adult_data/cig_smoking/index.htm'
search_text = "Current cigarette smoking was highest among"
smoking_rates_by_age_cdc = scrape_smoking_rates(url, search_text)

# Define a function to categorize age into age groups
def categorize_age_group(age):
    if age < 25:
        return '18–24 years'
    elif 25 <= age < 45:
        return '25–44 years'
    elif 45 <= age < 65:
        return '45–64 years'
    else:
        return '65 years and older'

# Assuming the dictionaries are provided like this:
smoking_rates_by_gender_cdc = {'men': 13.1, 'women': 10.1}
smoking_rates_by_age_cdc = {'18–24 years': 5.3, '25–44 years': 12.6, '45–64 years': 14.9, '65 years and older': 8.3}

# Broadcast the dictionaries
broadcast_smoking_rates_by_gender = spark.sparkContext.broadcast(smoking_rates_by_gender_cdc)
broadcast_smoking_rates_by_age = spark.sparkContext.broadcast(smoking_rates_by_age_cdc)

# Define the UDF to replace missing values based on age and gender
def replace_missing_smoke_rates(age, sex, smoke):
    age_group = categorize_age_group(age)  # Directly use the categorization here
    if smoke is None:
        gender_key = 'men' if sex == 1 else 'women'
        age_rate = broadcast_smoking_rates_by_age.value.get(age_group, 0)  # Default to 0 if not found
        if sex == 1:
            # Calculate the adjusted rate for men
            men_rate = broadcast_smoking_rates_by_gender.value.get('men', 1)
            women_rate = broadcast_smoking_rates_by_gender.value.get('women', 1)
            return age_rate * (men_rate / women_rate)
        else:
            # Use the age rate directly for women
            return age_rate
    else:
        return smoke

# Register the UDF with FloatType return type
replace_missing_smoke_rates_udf = udf(replace_missing_smoke_rates, FloatType())

# Update the DataFrame with the new 'smoke_updated' column
df = df.withColumn("smoke_cdc", replace_missing_smoke_rates_udf(col("age"), col("sex"), col("smoke")))

# Show the result
df.select("age", "sex", "smoke", "smoke_cdc").show()

"""Imputing painloc and painexer columns using mode imputation because they have categorical values"""

# Calculate the mode for the 'painloc' column
painloc_mode = df.groupBy("painloc").count().orderBy(F.desc("count")).first()['painloc']

# Replace missing values with the mode in 'painloc'
df = df.withColumn("painloc", F.when(col("painloc").isNull(), lit(painloc_mode)).otherwise(col("painloc")))

# Calculate the mode for the 'painexer' column
painexer_mode = df.groupBy("painexer").count().orderBy(F.desc("count")).first()['painexer']

# Replace missing values with the mode in 'painexer'
df = df.withColumn("painexer", F.when(col("painexer").isNull(), lit(painexer_mode)).otherwise(col("painexer")))

# Initialize Spark Session
spark = SparkSession.builder.appName("Transform Data").getOrCreate()

# Assuming df is your Spark DataFrame

# Convert string to float using cast
df = df.withColumn("smoke_abs", col("smoke_abs").cast("float"))
df = df.withColumn("smoke_cdc", col("smoke_cdc").cast("float"))

# Define a UDF for applying log transformation
def log_transform(value):
    if value is not None:
        return math.log(value / 100 + 10)
    else:
        return None

# Register the UDF
log_transform_udf = udf(log_transform, DoubleType())

# Apply the log transformation to 'smoke_abs' and 'smoke_cdc'
df = df.withColumn("smoke_abs", log_transform_udf("smoke_abs"))
df = df.withColumn("smoke_cdc", log_transform_udf("smoke_cdc"))

# Show the summary statistics to ensure the transformation was applied correctly
df.select("smoke_abs", "smoke_cdc").summary().show()

# Initialize Spark Session
spark = SparkSession.builder.appName("Prepare Data for Modeling").getOrCreate()

# Remove the original 'smoke' column
df = df.drop("smoke")
#Save the cleaned data
df.to_csv('cleaned_data2.csv', index=False)

def train_svm():
    df = pd.read_csv('cleaned_data1.csv')
    X = df.drop(['target'], axis=1)
    y = df['target']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    svm = SVC()
    svm.fit(X_train, y_train)
    svm_pred = svm.predict(X_test)
    svm_accuracy = accuracy_score(y_test, svm_pred)

    with open('svm_results.txt', 'w') as f:
        f.write(f'SVM Accuracy: {svm_accuracy}\n')

def train_lr():
    df = pd.read_csv('cleaned_data1.csv')
    X = df.drop(['target'], axis=1)
    y = df['target']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    lr = LogisticRegression()
    lr.fit(X_train, y_train)
    lr_pred = lr.predict(X_test)
    lr_accuracy = accuracy_score(y_test, lr_pred)

    with open('lr_results.txt', 'w') as f:
        f.write(f'Logistic Regression Accuracy: {lr_accuracy}\n')

def merge_results():
    with open('svm_results.txt', 'r') as f:
        svm_acc = float(f.readline().strip().split(': ')[1])
    with open('lr_results.txt', 'r') as f:
        lr_acc = float(f.readline().strip().split(': ')[1])

    best_model_acc = max(svm_acc, lr_acc)
    with open('best_model.txt', 'w') as f:
        f.write(f'Best Model Accuracy: {best_model_acc}\n')

def evaluate_on_test():
    with open('best_model.txt', 'r') as f:
        best_model_acc = float(f.readline().strip().split(': ')[1])
    # Perform evaluation on the test set using the best model
    print(f'Best Model Accuracy on Test Set: {best_model_acc}')

load_data_task = PythonOperator(
    task_id='load_data',
    python_callable=load_data,
    dag=dag,
)
clean_and_impute_task = PythonOperator(
    task_id='standard_feature_engineering',
    python_callable=clean_and_impute,
    dag=dag,
)
train_svm_task = PythonOperator(
    task_id='train_svm',
    python_callable=train_svm,
    dag=dag,
)
train_lr_task = PythonOperator(
    task_id='train_lr',
    python_callable=train_lr,
    dag=dag,
)

merge_results_task = PythonOperator(
    task_id='merge_results',
    python_callable=merge_results,
    dag=dag,
)

evaluate_on_test_task = PythonOperator(
    task_id='evaluate_on_test',
    python_callable=evaluate_on_test,
    dag=dag,
)

end = DummyOperator(task_id='end', dag=dag)

# Set task dependencies
start >> load_data_task >> clean_and_impute_task
clean_and_impute_task >> [train_svm_task, train_lr_task]
[train_svm_task, train_lr_task] >> merge_results_task >> evaluate_on_test_task >> end