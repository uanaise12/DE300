{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MlVJT-GMqioM"
      },
      "outputs": [],
      "source": [
        "# Install Java\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download and install Spark\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
        "\n",
        "# Install findspark, a Python library that makes it easier for Python to find Spark\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PySpark directly\n",
        "!pip install pyspark==3.5.1\n",
        "\n",
        "# Now you can import PySpark directly without needing findspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"Colab\").getOrCreate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNGzckG3sRBC",
        "outputId": "5fafffb8-aa3d-4054-e233-15f6a0e1920b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.5.1\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.5.1) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=3c3c048413c3a2cc888f2f51441a8a629b17f67bffcca0d6afd65f96257c33a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing libraries\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
        "from pyspark.ml.feature import Imputer\n",
        "from pyspark.sql.functions import when, count, broadcast\n",
        "from pyspark.sql.functions import col, expr, lit,udf\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import DoubleType, FloatType\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
        "import math\n"
      ],
      "metadata": {
        "id": "llPhB7YwuPen"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that the Spark session is created (as previously shown)\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"HeartDiseaseAnalysis\").getOrCreate()\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df1 = spark.read.csv(\"heart_disease(in).csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Show the first few rows of the DataFrame to verify it's loaded correctly\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cl7KY4-tvbKn",
        "outputId": "f4cd4f6b-4fce-46cb-9e69-21bd8d881124"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-------+--------+-------+-------+---+--------+---+----+-----+----+-----+---+----+-------+-------+-----+----------+-----+---+----+----+---+--------+-----+-------+--------+----+-------+--------+--------+--------+-----+--------+-----+-----+-------+-----+-----+------+---+-------+-------+------+------+------+------+----+-------+-------+-------+---+----+---+------+\n",
            "|age|sex|painloc|painexer|relrest|pncaden| cp|trestbps|htn|chol|smoke|cigs|years|fbs|  dm|famhist|restecg|ekgmo|ekgday(day|ekgyr|dig|prop|nitr|pro|diuretic|proto|thaldur|thaltime| met|thalach|thalrest|tpeakbps|tpeakbpd|dummy|trestbpd|exang|xhypo|oldpeak|slope|rldv5|rldv5e| ca|restckm|exerckm|restef|restwm|exeref|exerwm|thal|thalsev|thalpul|earlobe|cmo|cday|cyr|target|\n",
            "+---+---+-------+--------+-------+-------+---+--------+---+----+-----+----+-----+---+----+-------+-------+-----+----------+-----+---+----+----+---+--------+-----+-------+--------+----+-------+--------+--------+--------+-----+--------+-----+-----+-------+-----+-----+------+---+-------+-------+------+------+------+------+----+-------+-------+-------+---+----+---+------+\n",
            "| 63|  1|   NULL|    NULL|   NULL|   NULL|  1|     145|  1| 233| NULL|  50|   20|  1|NULL|      1|      2|    2|         3|   81|  0|   0|   0|  0|       0|    1|   10.5|     6.0|13.0|    150|      60|     190|      90|  145|      85|    0|    0|    2.3|    3| NULL|   172|  0|   NULL|   NULL|  NULL|  NULL|  NULL|  NULL|   6|   NULL|   NULL|   NULL|  2|  16| 81|     0|\n",
            "| 67|  1|   NULL|    NULL|   NULL|   NULL|  4|     160|  1| 286| NULL|  40|   40|  0|NULL|      1|      2|    3|         5|   81|  0|   1|   0|  0|       0|    1|    9.5|     6.0|13.0|    108|      64|     160|      90|  160|      90|    1|    0|    1.5|    2| NULL|   185|  3|   NULL|   NULL|  NULL|  NULL|  NULL|  NULL|   3|   NULL|   NULL|   NULL|  2|   5| 81|     1|\n",
            "| 67|  1|   NULL|    NULL|   NULL|   NULL|  4|     120|  1| 229| NULL|  20|   35|  0|NULL|      1|      2|    2|        19|   81|  0|   1|   0|  0|       0|    1|    8.5|     6.0|10.0|    129|      78|     140|      80|  120|      80|    1|    0|    2.6|    2| NULL|   150|  2|   NULL|   NULL|  NULL|  NULL|  NULL|  NULL|   7|   NULL|   NULL|   NULL|  2|  20| 81|     1|\n",
            "| 37|  1|   NULL|    NULL|   NULL|   NULL|  3|     130|  0| 250| NULL|   0|    0|  0|NULL|      1|      0|    2|        13|   81|  0|   1|   0|  0|       0|    1|   13.0|    13.0|17.0|    187|      84|     195|      68|  130|      78|    0|    0|    3.5|    3| NULL|   167|  0|   NULL|   NULL|  NULL|  NULL|  NULL|  NULL|   3|   NULL|   NULL|   NULL|  2|   4| 81|     0|\n",
            "| 41|  0|   NULL|    NULL|   NULL|   NULL|  2|     130|  1| 204| NULL|   0|    0|  0|NULL|      1|      2|    2|         7|   81|  0|   0|   0|  0|       0|    1|    7.0|    NULL| 9.0|    172|      71|     160|      74|  130|      86|    0|    0|    1.4|    1| NULL|    40|  0|   NULL|   NULL|  NULL|  NULL|  NULL|  NULL|   3|   NULL|   NULL|   NULL|  2|  18| 81|     0|\n",
            "| 56|  1|   NULL|    NULL|   NULL|   NULL|  2|     120|  1| 236| NULL|  20|   20|  0|NULL|      1|      0|    3|         9|   81|  1|   0|   0|  0|       0|    1|   11.3|    NULL|16.0|    178|      73|     165|      70|  120|      75|    0|    0|    0.8|    1| NULL|   127|  0|   NULL|   NULL|  NULL|  NULL|  NULL|  NULL|   3|   NULL|   NULL|   NULL|  3|  10| 81|     0|\n",
            "| 62|  0|   NULL|    NULL|   NULL|   NULL|  4|     140|  0| 268| NULL|   0|    0|  0|NULL|      1|      2|    1|        28|   81|  0|   0|   0|  0|       0|    1|    6.0|     6.0| 7.0|    160|      83|     180|      84|  140|     100|    0|    0|    3.6|    3| NULL|   122|  2|   NULL|   NULL|  NULL|  NULL|  NULL|  NULL|   3|   NULL|   NULL|   NULL|  2|   2| 81|     1|\n",
            "| 57|  0|   NULL|    NULL|   NULL|   NULL|  4|     120|  1| 354| NULL|   0|    0|  0|NULL|      1|      0|    7|        20|   81|  0|   0|   0|  0|       0|    1|    9.0|     6.0|10.0|    163|      84|     165|      80|  120|      80|    1|    0|    0.6|    1| NULL|   122|  0|   NULL|   NULL|  NULL|  NULL|  NULL|  NULL|   3|   NULL|   NULL|   NULL|  7|  21| 81|     0|\n",
            "| 63|  1|   NULL|    NULL|   NULL|   NULL|  4|     130|  1| 254| NULL|   0|    0|  0|NULL|      0|      2|    7|         2|   81|  0|   1|   1|  0|       0|    1|    8.0|     8.0| 9.0|    147|      75|     120|      70|  130|     105|    0|    0|    1.4|    2| NULL|    75|  1|   NULL|   NULL|  NULL|  NULL|  NULL|  NULL|   7|   NULL|   NULL|   NULL|  7|   3| 81|     1|\n",
            "| 53|  1|   NULL|    NULL|   NULL|   NULL|  4|     140|  0| 203| NULL|  20|   25|  1|NULL|      1|      2|    7|         3|   81|  0|   1|   0|  0|       1|    1|    5.5|     3.0| 7.0|    155|      86|     185|     120|  140|     100|    1|    0|    3.1|    3| NULL|    68|  0|   NULL|   NULL|  NULL|  NULL|  NULL|  NULL|   7|   NULL|   NULL|   NULL|  7|   6| 81|     1|\n",
            "| 57|  1|   NULL|    NULL|   NULL|   NULL|  4|     140|  0| 192| NULL|  75|   25|  0|NULL|      0|      0|    6|        30|   81|  0|   0|   0|  0|       0|    1|    8.2|    NULL| 9.0|    148|      86|     180|      75|  140|      90|    0|    0|    0.4|    2| NULL|   200|  0|   NULL|   NULL|  NULL|  NULL|  NULL|  NULL|   6|   NULL|   NULL|   NULL|  7|   1| 81|     0|\n",
            "| 56|  0|   NULL|    NULL|   NULL|   NULL|  2|     140|  0| 294| NULL|   0|    0|  0|NULL|      0|      2|    1|        16|   81|  0|   1|   1|  0|       0|    1|    4.5|     4.5| 7.0|    153|      85|     204|     118|  140|     100|    0|    0|    1.3|    2| NULL|   175|  0|   NULL|   NULL|  NULL|  NULL|  NULL|  NULL|   3|   NULL|   NULL|   NULL|  1|  19| 81|     0|\n",
            "| 56|  1|   NULL|    NULL|   NULL|   NULL|  3|     130|  0| 256| NULL|  30|   25|  1|   1|      1|      2|    1|        19|   81|  0|   0|   0|  0|       0|    1|   13.0|    NULL|16.0|    142|      74|     145|      87|  130|      90|    1|    0|    0.6|    2| NULL|   110|  1|   NULL|   NULL|  NULL|  NULL|  NULL|  NULL|   6|   NULL|   NULL|   NULL|  1|  20| 81|     1|\n",
            "| 44|  1|   NULL|    NULL|   NULL|   NULL|  2|     120|  1| 263| NULL|  50|   30|  0|NULL|      1|      0|    5|        20|   81|  0|   1|   0|  0|       0|    1|    9.3|    NULL|13.0|    173|      70|     165|      70|  120|      80|    0|    0|    0.0|    1| NULL|    49|  0|   NULL|   NULL|  NULL|  NULL|  NULL|  NULL|   7|   NULL|   NULL|   NULL|  5|  21| 81|     0|\n",
            "| 52|  1|   NULL|    NULL|   NULL|   NULL|  3|     172|  0| 199| NULL|  30|   35|  1|NULL|      0|      0|    6|         9|   81|  0|   0|   0|  0|       0|    1|   12.5|    NULL|18.0|    162|      91|     220|      96|  172|     100|    0|    0|    0.5|    1| NULL|   231|  0|   NULL|   NULL|  NULL|  NULL|  NULL|  NULL|   7|   NULL|   NULL|   NULL|  6|  10| 81|     0|\n",
            "| 57|  1|   NULL|    NULL|   NULL|   NULL|  3|     150|  1| 168| NULL|  15|   40|  0|NULL|      1|      0|    7|        13|   81|  0|   0|   1|  0|       0|    1|   11.0|    NULL|13.0|    174|      60|     192|      80|  150|      90|    0|    0|    1.6|    1| NULL|   150|  0|   NULL|   NULL|  NULL|  NULL|  NULL|  NULL|   3|   NULL|   NULL|   NULL|  7|  14| 81|     0|\n",
            "| 48|  1|   NULL|    NULL|   NULL|   NULL|  2|     110|  1| 229| NULL|   0|    0|  0|NULL|      0|      0|    7|        21|   81|  0|   1|   0|  0|       0|    1|    9.8|    NULL|13.0|    168|      75|     175|      80|  110|      80|    0|    0|    1.0|    3| NULL|   197|  0|   NULL|   NULL|  NULL|  NULL|  NULL|  NULL|   7|   NULL|   NULL|   NULL|  7|  21| 81|     1|\n",
            "| 54|  1|   NULL|    NULL|   NULL|   NULL|  4|     140|  1| 239| NULL|  20|   30|  0|NULL|      0|      0|    6|        24|   81|  0|   0|   0|  0|       1|    1|    7.8|     7.8|10.0|    160|      86|     180|      98|  140|     100|    0|    0|    1.2|    1| NULL|    85|  0|   NULL|   NULL|  NULL|  NULL|  NULL|  NULL|   3|   NULL|   NULL|   NULL|  6|  25| 81|     0|\n",
            "| 48|  0|   NULL|    NULL|   NULL|   NULL|  3|     130|  1| 275| NULL|   2|   20|  0|NULL|      1|      0|    6|         2|   81|  0|   0|   0|  0|       0|    1|   10.0|    NULL|13.0|    139|      62|     165|      75|  130|      80|    0|    0|    0.2|    1| NULL|   170|  0|   NULL|   NULL|  NULL|  NULL|  NULL|  NULL|   3|   NULL|   NULL|   NULL|  6|   3| 81|     0|\n",
            "| 49|  1|   NULL|    NULL|   NULL|   NULL|  2|     130|  1| 266| NULL|  25|   22|  0|NULL|      0|      0|    7|        24|   81|  0|   0|   0|  0|       0|    1|   12.0|    NULL|14.0|    171|      82|     135|      80|  130|      75|    0|    0|    0.6|    1| NULL|   127|  0|   NULL|   NULL|  NULL|  NULL|  NULL|  NULL|   3|   NULL|   NULL|   NULL|  7|  27| 81|     0|\n",
            "+---+---+-------+--------+-------+-------+---+--------+---+----+-----+----+-----+---+----+-------+-------+-----+----------+-----+---+----+----+---+--------+-----+-------+--------+----+-------+--------+--------+--------+-----+--------+-----+-----+-------+-----+-----+------+---+-------+-------+------+------+------+------+----+-------+-------+-------+---+----+---+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_keep = [\n",
        "    'age', 'sex', 'painloc', 'painexer', 'cp', 'trestbps', 'smoke', 'fbs',\n",
        "    'prop', 'nitr', 'pro', 'diuretic', 'thaldur', 'thalach', 'exang',\n",
        "    'oldpeak', 'slope', 'target'\n",
        "]\n",
        "# Select only the specified columns\n",
        "df = df1.select(*columns_to_keep)\n",
        "\n",
        "# Show the DataFrame to verify the columns\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofCfsDSAAcmo",
        "outputId": "cade9f83-43ad-433b-996f-4157ad2e7fd9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-------+--------+---+--------+-----+---+----+----+---+--------+-------+-------+-----+-------+-----+------+\n",
            "|age|sex|painloc|painexer| cp|trestbps|smoke|fbs|prop|nitr|pro|diuretic|thaldur|thalach|exang|oldpeak|slope|target|\n",
            "+---+---+-------+--------+---+--------+-----+---+----+----+---+--------+-------+-------+-----+-------+-----+------+\n",
            "| 63|  1|   NULL|    NULL|  1|     145| NULL|  1|   0|   0|  0|       0|   10.5|    150|    0|    2.3|    3|     0|\n",
            "| 67|  1|   NULL|    NULL|  4|     160| NULL|  0|   1|   0|  0|       0|    9.5|    108|    1|    1.5|    2|     1|\n",
            "| 67|  1|   NULL|    NULL|  4|     120| NULL|  0|   1|   0|  0|       0|    8.5|    129|    1|    2.6|    2|     1|\n",
            "| 37|  1|   NULL|    NULL|  3|     130| NULL|  0|   1|   0|  0|       0|   13.0|    187|    0|    3.5|    3|     0|\n",
            "| 41|  0|   NULL|    NULL|  2|     130| NULL|  0|   0|   0|  0|       0|    7.0|    172|    0|    1.4|    1|     0|\n",
            "| 56|  1|   NULL|    NULL|  2|     120| NULL|  0|   0|   0|  0|       0|   11.3|    178|    0|    0.8|    1|     0|\n",
            "| 62|  0|   NULL|    NULL|  4|     140| NULL|  0|   0|   0|  0|       0|    6.0|    160|    0|    3.6|    3|     1|\n",
            "| 57|  0|   NULL|    NULL|  4|     120| NULL|  0|   0|   0|  0|       0|    9.0|    163|    1|    0.6|    1|     0|\n",
            "| 63|  1|   NULL|    NULL|  4|     130| NULL|  0|   1|   1|  0|       0|    8.0|    147|    0|    1.4|    2|     1|\n",
            "| 53|  1|   NULL|    NULL|  4|     140| NULL|  1|   1|   0|  0|       1|    5.5|    155|    1|    3.1|    3|     1|\n",
            "| 57|  1|   NULL|    NULL|  4|     140| NULL|  0|   0|   0|  0|       0|    8.2|    148|    0|    0.4|    2|     0|\n",
            "| 56|  0|   NULL|    NULL|  2|     140| NULL|  0|   1|   1|  0|       0|    4.5|    153|    0|    1.3|    2|     0|\n",
            "| 56|  1|   NULL|    NULL|  3|     130| NULL|  1|   0|   0|  0|       0|   13.0|    142|    1|    0.6|    2|     1|\n",
            "| 44|  1|   NULL|    NULL|  2|     120| NULL|  0|   1|   0|  0|       0|    9.3|    173|    0|    0.0|    1|     0|\n",
            "| 52|  1|   NULL|    NULL|  3|     172| NULL|  1|   0|   0|  0|       0|   12.5|    162|    0|    0.5|    1|     0|\n",
            "| 57|  1|   NULL|    NULL|  3|     150| NULL|  0|   0|   1|  0|       0|   11.0|    174|    0|    1.6|    1|     0|\n",
            "| 48|  1|   NULL|    NULL|  2|     110| NULL|  0|   1|   0|  0|       0|    9.8|    168|    0|    1.0|    3|     1|\n",
            "| 54|  1|   NULL|    NULL|  4|     140| NULL|  0|   0|   0|  0|       1|    7.8|    160|    0|    1.2|    1|     0|\n",
            "| 48|  0|   NULL|    NULL|  3|     130| NULL|  0|   0|   0|  0|       0|   10.0|    139|    0|    0.2|    1|     0|\n",
            "| 49|  1|   NULL|    NULL|  2|     130| NULL|  0|   0|   0|  0|       0|   12.0|    171|    0|    0.6|    1|     0|\n",
            "+---+---+-------+--------+---+--------+-----+---+----+----+---+--------+-------+-------+-----+-------+-----+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing rows whose value of target is null first"
      ],
      "metadata": {
        "id": "nhpXBDkVA8QX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where the 'target' column has missing values\n",
        "df = df.na.drop(subset=[\"target\"])\n"
      ],
      "metadata": {
        "id": "_65ezlHtA_dO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imputing trestbps using median imputation since it's a numerical value"
      ],
      "metadata": {
        "id": "IKO8Y56QBQLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the DataFrame to include only rows where 'trestbps' is greater than or equal to 100\n",
        "# Then calculate the median of these filtered values\n",
        "median_df = df.filter(col('trestbps') >= 100)\n",
        "replacement_value = median_df.approxQuantile('trestbps', [0.5], 0.01)[0]  # 0.01 is the relative error, adjust as needed\n",
        "# Replace 'trestbps' values less than 100 or null with the median, otherwise keep original\n",
        "df = df.withColumn('trestbps', when((col('trestbps') < 100) | col('trestbps').isNull(), replacement_value).otherwise(col('trestbps')))\n",
        "df.select('trestbps').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoXio2IoBQyE",
        "outputId": "99e2c3a2-f01f-4b41-bfe5-c9a3b713f5b2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|trestbps|\n",
            "+--------+\n",
            "|   145.0|\n",
            "|   160.0|\n",
            "|   120.0|\n",
            "|   130.0|\n",
            "|   130.0|\n",
            "|   120.0|\n",
            "|   140.0|\n",
            "|   120.0|\n",
            "|   130.0|\n",
            "|   140.0|\n",
            "|   140.0|\n",
            "|   140.0|\n",
            "|   130.0|\n",
            "|   120.0|\n",
            "|   172.0|\n",
            "|   150.0|\n",
            "|   110.0|\n",
            "|   140.0|\n",
            "|   130.0|\n",
            "|   130.0|\n",
            "+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imputing oldpeak, excluding the thresholds"
      ],
      "metadata": {
        "id": "8yX9-4kvCMUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the DataFrame to include only rows where 'oldpeak' is between 0 and 4\n",
        "valid_range_df = df.filter((col('oldpeak') >= 0) & (col('oldpeak') <= 4))\n",
        "\n",
        "# Calculate the median of these filtered values\n",
        "replacement_value = valid_range_df.approxQuantile('oldpeak', [0.5], 0.01)[0]\n",
        "# Replace 'oldpeak' values outside the range 0-4 or null with the median, otherwise keep original\n",
        "df = df.withColumn('oldpeak', when((col('oldpeak') < 0) | (col('oldpeak') > 4) | col('oldpeak').isNull(), replacement_value).otherwise(col('oldpeak')))\n",
        "# Show the updated column to verify the replacement\n",
        "df.select('oldpeak').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AO7oHA6wB_2_",
        "outputId": "7242ecb9-13e0-44f8-9645-efab57d303d2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|oldpeak|\n",
            "+-------+\n",
            "|    2.3|\n",
            "|    1.5|\n",
            "|    2.6|\n",
            "|    3.5|\n",
            "|    1.4|\n",
            "|    0.8|\n",
            "|    3.6|\n",
            "|    0.6|\n",
            "|    1.4|\n",
            "|    3.1|\n",
            "|    0.4|\n",
            "|    1.3|\n",
            "|    0.6|\n",
            "|    0.0|\n",
            "|    0.5|\n",
            "|    1.6|\n",
            "|    1.0|\n",
            "|    1.2|\n",
            "|    0.2|\n",
            "|    0.6|\n",
            "+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imputing thaldur and thalach using median imputation (replacing in the null values)"
      ],
      "metadata": {
        "id": "ltEJCV3yDUBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the median for 'thaldur'\n",
        "thaldur_median = df.approxQuantile('thaldur', [0.5], 0.01)[0]\n",
        "\n",
        "# Calculate the median for 'thalach'\n",
        "thalach_median = df.approxQuantile('thalach', [0.5], 0.01)[0]\n",
        "# Replace missing values in 'thaldur' with the median\n",
        "df = df.na.fill({'thaldur': thaldur_median})\n",
        "\n",
        "# Replace missing values in 'thalach' with the median\n",
        "df = df.na.fill({'thalach': thalach_median})\n"
      ],
      "metadata": {
        "id": "I-V6CkxHDP3_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imputing fbs, prop, nitr, pro, diuretic by\n",
        "replacing the missing values and values greater than 1."
      ],
      "metadata": {
        "id": "YPeOAmyvEP0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_values(df, column):\n",
        "    \"\"\" Replace values greater than 1 with 1 in the specified column \"\"\"\n",
        "    return df.withColumn(column, when(col(column) > 1, 1).otherwise(col(column)))\n",
        "\n",
        "def calculate_mode(df, column):\n",
        "    \"\"\" Calculate the mode (most frequent value) of the specified column \"\"\"\n",
        "    mode_value = df.groupBy(column).count().orderBy('count', ascending=False).first()[0]\n",
        "    return mode_value\n",
        "\n",
        "columns = ['fbs', 'prop', 'nitr', 'pro', 'diuretic']\n",
        "for column in columns:\n",
        "    # Replace values greater than 1 with 1\n",
        "    df = replace_values(df, column)\n",
        "\n",
        "    # Calculate the mode of the column\n",
        "    mode_value = calculate_mode(df, column)\n",
        "\n",
        "    # Replace missing values with the mode\n",
        "    df = df.na.fill({column: mode_value})"
      ],
      "metadata": {
        "id": "EAa84jQvES1j"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imputing exang and slope by replacing the missing values with the mode."
      ],
      "metadata": {
        "id": "Sc-ZbGSYFytx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mode(df, column_name):\n",
        "    \"\"\" Calculate the mode (most common value) for the specified column \"\"\"\n",
        "    # Group by the column and count occurrences, then order by count descending and take the first\n",
        "    mode_value = df.groupBy(column_name).count().orderBy(col(\"count\").desc()).first()[0]\n",
        "    return mode_value\n",
        "\n",
        "# Calculate mode for 'exang'\n",
        "exang_mode = calculate_mode(df, 'exang')\n",
        "\n",
        "# Calculate mode for 'slope'\n",
        "slope_mode = calculate_mode(df, 'slope')\n",
        "\n",
        "# Replace missing values with the mode\n",
        "from pyspark.sql.functions import when\n",
        "df = df.withColumn(\"exang\", when(col(\"exang\").isNull(), exang_mode).otherwise(col(\"exang\")))\n",
        "df = df.withColumn(\"slope\", when(col(\"slope\").isNull(), slope_mode).otherwise(col(\"slope\")))\n"
      ],
      "metadata": {
        "id": "UT5tzLAWF01p"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning out the age column so that it can be used for finding out the corresponding age range.\n",
        "\n",
        "We will use median imputation"
      ],
      "metadata": {
        "id": "rB8N5xxtJDE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Attempt to cast the 'age' column to double type (float in Spark SQL)\n",
        "# Invalid entries will be turned into nulls\n",
        "df = df.withColumn(\"age\", col(\"age\").cast(DoubleType()))\n",
        "# Calculate the median age, ignoring nulls by default\n",
        "median_age = df.approxQuantile(\"age\", [0.5], 0.01)[0]\n",
        "# Fill NaN values in the 'age' column with the median age\n",
        "df = df.na.fill({\"age\": median_age})\n",
        "\n"
      ],
      "metadata": {
        "id": "s5uvaSEHJEAY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scraping for the smoke column\n",
        "\n",
        "Using https://www.abs.gov.au/statistics/health/health-conditions-and-risks/smoking/latest-release"
      ],
      "metadata": {
        "id": "24Ae6p25JrRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# URL of the webpage\n",
        "url = 'https://www.abs.gov.au/statistics/health/health-conditions-and-risks/smoking/latest-release'\n",
        "\n",
        "# Send GET request\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find all tables with a specific caption\n",
        "tables = soup.find_all('table', class_='responsive-enabled')\n",
        "table = None\n",
        "for tbl in tables:\n",
        "    caption = tbl.find('caption')\n",
        "    if caption and 'Proportion of people 15 years and over who were current daily smokers by age, 2011–12 to 2022' in caption.text:\n",
        "        table = tbl\n",
        "        break\n",
        "\n",
        "# Check if the correct table is found\n",
        "if not table:\n",
        "    print(\"The specified table was not found.\")\n",
        "else:\n",
        "    # Initialize a list to store each row's data\n",
        "    data = []\n",
        "\n",
        "    # Iterate over each row in the table body\n",
        "    for row in table.find_all('tr')[1:]:  # Skip the header row\n",
        "        cols = row.find_all('td')\n",
        "        age_group = row.th.text.strip() if row.th else 'No Age Group'  # Ensuring to get the age group if available\n",
        "        row_data = [age_group] + [col.text.strip() for col in cols]\n",
        "        data.append(row_data)\n",
        "\n",
        "    # Define column headers based on the table's structure\n",
        "    columns = ['Age Group', '2011-12 (%)', 'CI Low (2011-12)', 'CI High (2011-12)',\n",
        "               '2014-15 (%)', 'CI Low (2014-15)', 'CI High (2014-15)',\n",
        "               '2017-18 (%)', 'CI Low (2017-18)', 'CI High (2017-18)',\n",
        "               '2022 (%)', 'CI Low (2022)', 'CI High (2022)']\n",
        "\n",
        "    # Create a DataFrame\n",
        "    smoking_data_df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "    # Display the first few rows of the DataFrame to check\n",
        "    print(smoking_data_df['Age Group'],smoking_data_df['2022 (%)'])\n",
        "    print (type(smoking_data_df['Age Group'][1]))\n",
        "    print (type(smoking_data_df['2022 (%)'][1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu4E5i5uJu7d",
        "outputId": "753e2f17-01a5-4bc6-914c-7d497bc1be4b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                15–17\n",
            "1                18–24\n",
            "2                25–34\n",
            "3                35–44\n",
            "4                45–54\n",
            "5                55–64\n",
            "6                65–74\n",
            "7    75 years and over\n",
            "Name: Age Group, dtype: object 0     1.6\n",
            "1     7.3\n",
            "2    10.9\n",
            "3    10.9\n",
            "4    13.8\n",
            "5    14.9\n",
            "6     8.7\n",
            "7     2.9\n",
            "Name: 2022 (%), dtype: object\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "Column<'smoke'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf, col\n",
        "from pyspark.sql.types import StringType, FloatType\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"Smoking Data Analysis\").getOrCreate()\n",
        "\n",
        "# Convert pandas DataFrame to Spark DataFrame if not already done\n",
        "smoking_data_spark_df = spark.createDataFrame(smoking_data_df)\n",
        "\n",
        "# Define a UDF to replace NaN in 'smoke' based on age groups\n",
        "@udf(StringType())  # Specify the return type of the UDF\n",
        "def get_smoking_rate(age, smoke):\n",
        "    age_groups = broadcast_age_groups.value\n",
        "    if smoke is None:\n",
        "        age = int(age)\n",
        "        for age_range, rate in age_groups.items():\n",
        "            if age_range == '75 years and over':\n",
        "                if age >= 75:\n",
        "                    return rate\n",
        "            else:\n",
        "                low, high = map(int, age_range.split('–'))\n",
        "                if low <= age <= high:\n",
        "                    return rate\n",
        "        return None\n",
        "    else:\n",
        "        return smoke\n",
        "\n",
        "# Collect age groups and rates into a dictionary for broadcast to workers\n",
        "age_groups_rates = {row['Age Group']: row['2022 (%)'] for row in smoking_data_spark_df.collect()}\n",
        "broadcast_age_groups = spark.sparkContext.broadcast(age_groups_rates)\n",
        "\n",
        "# Apply the UDF to the DataFrame\n",
        "df = df.withColumn(\"smoke_abs\", get_smoking_rate(col(\"age\"), col(\"smoke\")))\n",
        "\n",
        "# Show the result\n",
        "df.select(\"age\", \"smoke\", \"smoke_abs\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yftlPmeL1T22",
        "outputId": "24f204de-5730-4a1d-c225-3bc73b341ac8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+---------+\n",
            "| age|smoke|smoke_abs|\n",
            "+----+-----+---------+\n",
            "|63.0| NULL|     14.9|\n",
            "|67.0| NULL|      8.7|\n",
            "|67.0| NULL|      8.7|\n",
            "|37.0| NULL|     10.9|\n",
            "|41.0| NULL|     10.9|\n",
            "|56.0| NULL|     14.9|\n",
            "|62.0| NULL|     14.9|\n",
            "|57.0| NULL|     14.9|\n",
            "|63.0| NULL|     14.9|\n",
            "|53.0| NULL|     13.8|\n",
            "|57.0| NULL|     14.9|\n",
            "|56.0| NULL|     14.9|\n",
            "|56.0| NULL|     14.9|\n",
            "|44.0| NULL|     10.9|\n",
            "|52.0| NULL|     13.8|\n",
            "|57.0| NULL|     14.9|\n",
            "|48.0| NULL|     13.8|\n",
            "|54.0| NULL|     13.8|\n",
            "|48.0| NULL|     13.8|\n",
            "|49.0| NULL|     13.8|\n",
            "+----+-----+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imputing the sex column using mode imputation"
      ],
      "metadata": {
        "id": "R9wl41yTMZZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mode of the 'sex' column\n",
        "mode_sex_df = df.groupBy(\"sex\").count().orderBy(col(\"count\").desc()).limit(1)\n",
        "mode_sex = mode_sex_df.collect()[0]['sex']\n",
        "# Replace missing values in the 'sex' column with the mode\n",
        "df = df.na.fill({'sex': mode_sex})"
      ],
      "metadata": {
        "id": "m9_IRofeMdYQ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scraping CDC data by sex"
      ],
      "metadata": {
        "id": "iPqt4fC2RkRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL of the webpage\n",
        "url = 'https://www.cdc.gov/tobacco/data_statistics/fact_sheets/adult_data/cig_smoking/index.htm'\n",
        "\n",
        "# Send GET request\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Initialize a dictionary to store the gender-specific smoking rates\n",
        "smoking_rates_by_gender_cdc = {}\n",
        "\n",
        "# Find the relevant card containing smoking data by gender\n",
        "cards = soup.find_all('div', class_='card-body')\n",
        "gender_card = next((card for card in cards if \"Current cigarette smoking was higher among men than women\" in card.text), None)\n",
        "\n",
        "# Process the relevant data if found\n",
        "if gender_card:\n",
        "    list_items = gender_card.find_all('li')\n",
        "    for item in list_items:\n",
        "        text = item.get_text().strip()\n",
        "        gender = 'men' if 'adult men' in text else 'women'\n",
        "        percentage = text.split('(')[-1].rstrip('%)').strip()\n",
        "        smoking_rates_by_gender_cdc[gender] = float(percentage)\n",
        "else:\n",
        "    print(\"Gender-specific smoking section not found.\")\n",
        "\n",
        "# Output the dictionary to verify the extracted data\n",
        "print(smoking_rates_by_gender_cdc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-ciAZdPRrqZ",
        "outputId": "a13afd67-f7a7-48a0-967a-8b26bd0fdcca"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'men': 13.1, 'women': 10.1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_smoking_rates(url, search_text):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    smoking_rates = {}\n",
        "\n",
        "    cards = soup.find_all('div', class_='card-body')\n",
        "    age_card = next((card for card in cards if search_text in card.text), None)\n",
        "\n",
        "    if age_card:\n",
        "        list_items = age_card.find_all('li')\n",
        "        for item in list_items:\n",
        "            text = item.get_text().strip()\n",
        "            age_range = text.split('adults aged')[1].split('(')[0].strip()\n",
        "            percentage = text.split('(')[-1].rstrip('%)').strip()\n",
        "            smoking_rates[age_range] = float(percentage)\n",
        "    else:\n",
        "        print(f\"{search_text} section not found.\")\n",
        "\n",
        "    return smoking_rates\n",
        "\n",
        "url = 'https://www.cdc.gov/tobacco/data_statistics/fact_sheets/adult_data/cig_smoking/index.htm'\n",
        "search_text = \"Current cigarette smoking was highest among\"\n",
        "smoking_rates_by_age_cdc = scrape_smoking_rates(url, search_text)\n",
        "print(smoking_rates_by_age_cdc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ll_LkJr6T9xW",
        "outputId": "9f97d6e0-10b5-4e3e-ef4b-911e1e32cfee"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'18–24 years': 5.3, '25–44 years': 12.6, '45–64 years': 14.9, '65 years and older': 8.3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf, col\n",
        "from pyspark.sql.types import FloatType, StringType\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"Smoking Data Update\").getOrCreate()\n",
        "\n",
        "# Define a function to categorize age into age groups\n",
        "def categorize_age_group(age):\n",
        "    if age < 25:\n",
        "        return '18–24 years'\n",
        "    elif 25 <= age < 45:\n",
        "        return '25–44 years'\n",
        "    elif 45 <= age < 65:\n",
        "        return '45–64 years'\n",
        "    else:\n",
        "        return '65 years and older'\n",
        "\n",
        "# Assuming the dictionaries are provided like this:\n",
        "smoking_rates_by_gender_cdc = {'men': 13.1, 'women': 10.1}\n",
        "smoking_rates_by_age_cdc = {'18–24 years': 5.3, '25–44 years': 12.6, '45–64 years': 14.9, '65 years and older': 8.3}\n",
        "\n",
        "# Broadcast the dictionaries\n",
        "broadcast_smoking_rates_by_gender = spark.sparkContext.broadcast(smoking_rates_by_gender_cdc)\n",
        "broadcast_smoking_rates_by_age = spark.sparkContext.broadcast(smoking_rates_by_age_cdc)\n",
        "\n",
        "# Define the UDF to replace missing values based on age and gender\n",
        "def replace_missing_smoke_rates(age, sex, smoke):\n",
        "    age_group = categorize_age_group(age)  # Directly use the categorization here\n",
        "    if smoke is None:\n",
        "        gender_key = 'men' if sex == 1 else 'women'\n",
        "        age_rate = broadcast_smoking_rates_by_age.value.get(age_group, 0)  # Default to 0 if not found\n",
        "        if sex == 1:\n",
        "            # Calculate the adjusted rate for men\n",
        "            men_rate = broadcast_smoking_rates_by_gender.value.get('men', 1)\n",
        "            women_rate = broadcast_smoking_rates_by_gender.value.get('women', 1)\n",
        "            return age_rate * (men_rate / women_rate)\n",
        "        else:\n",
        "            # Use the age rate directly for women\n",
        "            return age_rate\n",
        "    else:\n",
        "        return smoke\n",
        "\n",
        "# Register the UDF with FloatType return type\n",
        "replace_missing_smoke_rates_udf = udf(replace_missing_smoke_rates, FloatType())\n",
        "\n",
        "# Update the DataFrame with the new 'smoke_updated' column\n",
        "df = df.withColumn(\"smoke_cdc\", replace_missing_smoke_rates_udf(col(\"age\"), col(\"sex\"), col(\"smoke\")))\n",
        "\n",
        "# Show the result\n",
        "df.select(\"age\", \"sex\", \"smoke\", \"smoke_cdc\").show()\n"
      ],
      "metadata": {
        "id": "7t2xh7ANZPDl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96b3b319-fb42-4778-ab94-f7451ece09b0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+-----+---------+\n",
            "| age|sex|smoke|smoke_cdc|\n",
            "+----+---+-----+---------+\n",
            "|63.0|  1| NULL|19.325743|\n",
            "|67.0|  1| NULL|10.765347|\n",
            "|67.0|  1| NULL|10.765347|\n",
            "|37.0|  1| NULL|16.342575|\n",
            "|41.0|  0| NULL|     12.6|\n",
            "|56.0|  1| NULL|19.325743|\n",
            "|62.0|  0| NULL|     14.9|\n",
            "|57.0|  0| NULL|     14.9|\n",
            "|63.0|  1| NULL|19.325743|\n",
            "|53.0|  1| NULL|19.325743|\n",
            "|57.0|  1| NULL|19.325743|\n",
            "|56.0|  0| NULL|     14.9|\n",
            "|56.0|  1| NULL|19.325743|\n",
            "|44.0|  1| NULL|16.342575|\n",
            "|52.0|  1| NULL|19.325743|\n",
            "|57.0|  1| NULL|19.325743|\n",
            "|48.0|  1| NULL|19.325743|\n",
            "|54.0|  1| NULL|19.325743|\n",
            "|48.0|  0| NULL|     14.9|\n",
            "|49.0|  1| NULL|19.325743|\n",
            "+----+---+-----+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imputing painloc and painexer columns using mode imputation because they have categorical values"
      ],
      "metadata": {
        "id": "IBD_qlnR7lhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"Data Cleaning\").getOrCreate()\n",
        "\n",
        "# Assuming df is your Spark DataFrame\n",
        "\n",
        "# Calculate the mode for the 'painloc' column\n",
        "painloc_mode = df.groupBy(\"painloc\").count().orderBy(F.desc(\"count\")).first()['painloc']\n",
        "\n",
        "# Replace missing values with the mode in 'painloc'\n",
        "df = df.withColumn(\"painloc\", F.when(col(\"painloc\").isNull(), lit(painloc_mode)).otherwise(col(\"painloc\")))\n",
        "\n",
        "# Calculate the mode for the 'painexer' column\n",
        "painexer_mode = df.groupBy(\"painexer\").count().orderBy(F.desc(\"count\")).first()['painexer']\n",
        "\n",
        "# Replace missing values with the mode in 'painexer'\n",
        "df = df.withColumn(\"painexer\", F.when(col(\"painexer\").isNull(), lit(painexer_mode)).otherwise(col(\"painexer\")))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U6zNbppC7mPe"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"Transform Data\").getOrCreate()\n",
        "\n",
        "# Assuming df is your Spark DataFrame\n",
        "\n",
        "# Convert string to float using cast\n",
        "df = df.withColumn(\"smoke_abs\", col(\"smoke_abs\").cast(\"float\"))\n",
        "df = df.withColumn(\"smoke_cdc\", col(\"smoke_cdc\").cast(\"float\"))\n",
        "\n",
        "# Define a UDF for applying log transformation\n",
        "def log_transform(value):\n",
        "    if value is not None:\n",
        "        return math.log(value / 100 + 10)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Register the UDF\n",
        "log_transform_udf = udf(log_transform, DoubleType())\n",
        "\n",
        "# Apply the log transformation to 'smoke_abs' and 'smoke_cdc'\n",
        "df = df.withColumn(\"smoke_abs\", log_transform_udf(\"smoke_abs\"))\n",
        "df = df.withColumn(\"smoke_cdc\", log_transform_udf(\"smoke_cdc\"))\n",
        "\n",
        "# Show the summary statistics to ensure the transformation was applied correctly\n",
        "df.select(\"smoke_abs\", \"smoke_cdc\").summary().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTmU87iU787o",
        "outputId": "dc7d56b5-51e5-41a4-f628-05d6e6886383"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+--------------------+\n",
            "|summary|           smoke_abs|           smoke_cdc|\n",
            "+-------+--------------------+--------------------+\n",
            "|  count|                 899|                 899|\n",
            "|   mean|  2.3048947590206144|   2.304897557806103|\n",
            "| stddev|5.691948627879489E-6|7.495973519195899E-6|\n",
            "|    min|   2.304885031232229|   2.304885031232229|\n",
            "|    25%|   2.304886028386382|   2.304886028386382|\n",
            "|    50%|   2.304898705383064|  2.3049012044170367|\n",
            "|    75%|   2.304898705383064|   2.304904128511966|\n",
            "|    max|  2.3048997872043624|   2.304904128511966|\n",
            "+-------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"Prepare Data for Modeling\").getOrCreate()\n",
        "\n",
        "# Remove the original 'smoke' column\n",
        "df = df.drop(\"smoke\")\n",
        "\n",
        "# Display the DataFrame structure and preview to verify the column has been dropped\n",
        "df.show()\n",
        "df.printSchema()\n",
        "\n",
        "# Check for NaN values in each column\n",
        "nan_counts = df.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in df.columns])\n",
        "\n",
        "# Print the counts of NaN values\n",
        "nan_counts.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bsh7RsqK8SbF",
        "outputId": "c6134149-926d-44a0-f1b2-2fb5b1c6d26f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+-------+--------+---+--------+---+----+----+---+--------+-------+-------+-----+-------+-----+------+------------------+------------------+-------------+------------------+\n",
            "| age|sex|painloc|painexer| cp|trestbps|fbs|prop|nitr|pro|diuretic|thaldur|thalach|exang|oldpeak|slope|target|         smoke_abs|         age_group|smoke_updated|         smoke_cdc|\n",
            "+----+---+-------+--------+---+--------+---+----+----+---+--------+-------+-------+-----+-------+-----+------+------------------+------------------+-------------+------------------+\n",
            "|63.0|  1|      1|       1|  1|   145.0|  1|   0|   0|  0|       0|   10.5|    150|    0|    2.3|    3|     0|2.3048997872043624|       45–64 years|    19.325743| 2.304904128511966|\n",
            "|67.0|  1|      1|       1|  4|   160.0|  0|   1|   0|  0|       0|    9.5|    108|    1|    1.5|    2|     1|2.3048936735190133|65 years and older|    10.765347|2.3048957144312814|\n",
            "|67.0|  1|      1|       1|  4|   120.0|  0|   1|   0|  0|       0|    8.5|    129|    1|    2.6|    2|     1|2.3048936735190133|65 years and older|    10.765347|2.3048957144312814|\n",
            "|37.0|  1|      1|       1|  3|   130.0|  0|   1|   0|  0|       0|   13.0|    187|    0|    3.5|    3|     0| 2.304895847161795|       25–44 years|    16.342575|2.3049012044170367|\n",
            "|41.0|  0|      1|       1|  2|   130.0|  0|   0|   0|  0|       0|    7.0|    172|    0|    1.4|    1|     0| 2.304895847161795|       25–44 years|         12.6|2.3048975236560865|\n",
            "|56.0|  1|      1|       1|  2|   120.0|  0|   0|   0|  0|       0|   11.3|    178|    0|    0.8|    1|     0|2.3048997872043624|       45–64 years|    19.325743| 2.304904128511966|\n",
            "|62.0|  0|      1|       1|  4|   140.0|  0|   0|   0|  0|       0|    6.0|    160|    0|    3.6|    3|     1|2.3048997872043624|       45–64 years|         14.9|2.3048997872043624|\n",
            "|57.0|  0|      1|       1|  4|   120.0|  0|   0|   0|  0|       0|    9.0|    163|    1|    0.6|    1|     0|2.3048997872043624|       45–64 years|         14.9|2.3048997872043624|\n",
            "|63.0|  1|      1|       1|  4|   130.0|  0|   1|   1|  0|       0|    8.0|    147|    0|    1.4|    2|     1|2.3048997872043624|       45–64 years|    19.325743| 2.304904128511966|\n",
            "|53.0|  1|      1|       1|  4|   140.0|  1|   1|   0|  0|       1|    5.5|    155|    1|    3.1|    3|     1| 2.304898705383064|       45–64 years|    19.325743| 2.304904128511966|\n",
            "|57.0|  1|      1|       1|  4|   140.0|  0|   0|   0|  0|       0|    8.2|    148|    0|    0.4|    2|     0|2.3048997872043624|       45–64 years|    19.325743| 2.304904128511966|\n",
            "|56.0|  0|      1|       1|  2|   140.0|  0|   1|   1|  0|       0|    4.5|    153|    0|    1.3|    2|     0|2.3048997872043624|       45–64 years|         14.9|2.3048997872043624|\n",
            "|56.0|  1|      1|       1|  3|   130.0|  1|   0|   0|  0|       0|   13.0|    142|    1|    0.6|    2|     1|2.3048997872043624|       45–64 years|    19.325743| 2.304904128511966|\n",
            "|44.0|  1|      1|       1|  2|   120.0|  0|   1|   0|  0|       0|    9.3|    173|    0|    0.0|    1|     0| 2.304895847161795|       25–44 years|    16.342575|2.3049012044170367|\n",
            "|52.0|  1|      1|       1|  3|   172.0|  1|   0|   0|  0|       0|   12.5|    162|    0|    0.5|    1|     0| 2.304898705383064|       45–64 years|    19.325743| 2.304904128511966|\n",
            "|57.0|  1|      1|       1|  3|   150.0|  0|   0|   1|  0|       0|   11.0|    174|    0|    1.6|    1|     0|2.3048997872043624|       45–64 years|    19.325743| 2.304904128511966|\n",
            "|48.0|  1|      1|       1|  2|   110.0|  0|   1|   0|  0|       0|    9.8|    168|    0|    1.0|    3|     1| 2.304898705383064|       45–64 years|    19.325743| 2.304904128511966|\n",
            "|54.0|  1|      1|       1|  4|   140.0|  0|   0|   0|  0|       1|    7.8|    160|    0|    1.2|    1|     0| 2.304898705383064|       45–64 years|    19.325743| 2.304904128511966|\n",
            "|48.0|  0|      1|       1|  3|   130.0|  0|   0|   0|  0|       0|   10.0|    139|    0|    0.2|    1|     0| 2.304898705383064|       45–64 years|         14.9|2.3048997872043624|\n",
            "|49.0|  1|      1|       1|  2|   130.0|  0|   0|   0|  0|       0|   12.0|    171|    0|    0.6|    1|     0| 2.304898705383064|       45–64 years|    19.325743| 2.304904128511966|\n",
            "+----+---+-------+--------+---+--------+---+----+----+---+--------+-------+-------+-----+-------+-----+------+------------------+------------------+-------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- age: double (nullable = false)\n",
            " |-- sex: integer (nullable = false)\n",
            " |-- painloc: integer (nullable = true)\n",
            " |-- painexer: integer (nullable = true)\n",
            " |-- cp: integer (nullable = true)\n",
            " |-- trestbps: double (nullable = true)\n",
            " |-- fbs: integer (nullable = false)\n",
            " |-- prop: integer (nullable = false)\n",
            " |-- nitr: integer (nullable = false)\n",
            " |-- pro: integer (nullable = false)\n",
            " |-- diuretic: integer (nullable = false)\n",
            " |-- thaldur: double (nullable = false)\n",
            " |-- thalach: integer (nullable = true)\n",
            " |-- exang: integer (nullable = true)\n",
            " |-- oldpeak: double (nullable = true)\n",
            " |-- slope: integer (nullable = true)\n",
            " |-- target: integer (nullable = true)\n",
            " |-- smoke_abs: double (nullable = true)\n",
            " |-- age_group: string (nullable = true)\n",
            " |-- smoke_updated: float (nullable = true)\n",
            " |-- smoke_cdc: double (nullable = true)\n",
            "\n",
            "+---+---+-------+--------+---+--------+---+----+----+---+--------+-------+-------+-----+-------+-----+------+---------+---------+-------------+---------+\n",
            "|age|sex|painloc|painexer| cp|trestbps|fbs|prop|nitr|pro|diuretic|thaldur|thalach|exang|oldpeak|slope|target|smoke_abs|age_group|smoke_updated|smoke_cdc|\n",
            "+---+---+-------+--------+---+--------+---+----+----+---+--------+-------+-------+-----+-------+-----+------+---------+---------+-------------+---------+\n",
            "|  0|  0|      0|       0|  0|       0|  0|   0|   0|  0|       0|      0|      0|    0|      0|    0|     0|        0|        0|          230|      230|\n",
            "+---+---+-------+--------+---+--------+---+----+----+---+--------+-------+-------+-----+-------+-----+------+---------+---------+-------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting data 90 - 10"
      ],
      "metadata": {
        "id": "fOjkEQqo9aq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"ML Data Split\").getOrCreate()\n",
        "\n",
        "# Assuming 'df' is your DataFrame and 'target' is your target column\n",
        "# Add a column for random numbers\n",
        "df = df.withColumn(\"random\", F.rand(seed=42))\n",
        "\n",
        "# Window specification for stratified sampling\n",
        "windowSpec = Window.partitionBy(\"target\").orderBy(\"random\")\n",
        "\n",
        "# Adding a row number within each partition of 'target'\n",
        "df = df.withColumn(\"row_number\", F.row_number().over(windowSpec))\n",
        "\n",
        "# Count the number of each 'target' category\n",
        "target_counts = df.groupBy(\"target\").count().collect()\n",
        "fractions = {row['target']: 0.9 for row in target_counts}  # 90% for training\n",
        "\n",
        "# Applying the sampling fraction based on 'target'\n",
        "train_df = df.sampleBy(\"target\", fractions, seed=42)\n",
        "test_df = df.join(train_df, on=df.columns, how=\"left_anti\")  # Get the rows not in the training set\n",
        "\n",
        "# Optionally remove the extra columns if you don't need them anymore\n",
        "train_df = train_df.drop(\"random\", \"row_number\")\n",
        "test_df = test_df.drop(\"random\", \"row_number\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Rf0dSVSz9Pl4"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "List the models we'll try:"
      ],
      "metadata": {
        "id": "82Ztg1pC_LCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier\n",
        "from pyspark.sql.functions import col, isnan, when, count\n",
        "\n",
        "# Initialize PySpark models\n",
        "log_reg = LogisticRegression(featuresCol='features', labelCol='target')\n",
        "dec_tree = DecisionTreeClassifier(featuresCol='features', labelCol='target')\n",
        "rand_forest = RandomForestClassifier(featuresCol='features', labelCol='target')\n",
        "\n",
        "# List of models and their names\n",
        "models = [log_reg, dec_tree, rand_forest]\n",
        "model_names = ['Logistic Regression', 'Decision Tree', 'Random Forest']\n",
        "\n",
        "# Assuming 'df' is your DataFrame and 'target' is your target column\n",
        "# First, let's split the data into features and target similar to X and y in your scikit-learn approach\n",
        "# Normally, you'd need a VectorAssembler here to combine feature columns into a single features vector\n",
        "# Assuming that 'features' column already exists or you've previously created it using VectorAssembler\n",
        "\n",
        "# Splitting the data into training and testing sets (90% training, 10% testing)\n",
        "train_df, test_df = df.randomSplit([0.9, 0.1], seed=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "bqVyKMY8_OIQ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier, GBTClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler, Imputer\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Prepare data: impute missing values and assemble features\n",
        "numeric_features = [c for c, t in df.dtypes if c != 'target' and t != 'string']\n",
        "imputer = Imputer(inputCols=numeric_features, outputCols=[f\"{c}_imputed\" for c in numeric_features])\n",
        "df = imputer.setStrategy(\"median\").fit(df).transform(df)\n",
        "df.cache()  # Cache the DataFrame after imputation\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[f\"{c}_imputed\" for c in numeric_features],\n",
        "    outputCol=\"features\",\n",
        "    handleInvalid=\"keep\"\n",
        ")\n",
        "df_assembled = assembler.transform(df)\n",
        "train_df, test_df = df_assembled.randomSplit([0.9, 0.1], seed=42)\n",
        "\n",
        "# Define classifiers and parameter grid with reduced options\n",
        "models = {\n",
        "    'RandomForest': RandomForestClassifier(featuresCol=\"features\", labelCol=\"target\"),\n",
        "    'DecisionTree': DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"target\"),\n",
        "    'GBT': GBTClassifier(featuresCol=\"features\", labelCol=\"target\")\n",
        "}\n",
        "\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(models['RandomForest'].numTrees, [100]) \\\n",
        "    .addGrid(models['RandomForest'].maxDepth, [10]) \\\n",
        "    .build()\n",
        "\n",
        "# Setup cross-validation\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"target\")\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    pipeline = Pipeline(stages=[model])\n",
        "    crossval = CrossValidator(estimator=pipeline,\n",
        "                              estimatorParamMaps=paramGrid,\n",
        "                              evaluator=evaluator,\n",
        "                              numFolds=5)  # Maintaining 5 folds\n",
        "    cvModel = crossval.fit(train_df)\n",
        "    bestModel = cvModel.bestModel\n",
        "    predictions = bestModel.transform(test_df)\n",
        "    test_score = evaluator.evaluate(predictions)\n",
        "    results[name] = {\n",
        "        'Best Model': bestModel,\n",
        "        'CV Score': cvModel.avgMetrics[0],\n",
        "        'Test Score': test_score\n",
        "    }\n",
        "\n",
        "# Print results\n",
        "for result in results:\n",
        "    print(f\"{result}: CV Score: {results[result]['CV Score']}, Test Score: {results[result]['Test Score']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoEB6Vzk_z-A",
        "outputId": "ca56e071-57cf-4452-ce59-59599459fe24"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForest: CV Score: 0.9016813414467526, Test Score: 0.8968095712861416\n",
            "DecisionTree: CV Score: 0.7431830608363378, Test Score: 0.7791625124626121\n",
            "GBT: CV Score: 0.9071413226933194, Test Score: 0.9032901296111666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the results above, Gradient-Boosted Trees classifier (GBT) is the best classifier for the given dataset. For future predictions, I would use this model."
      ],
      "metadata": {
        "id": "ZIMCXgdxt0zh"
      }
    }
  ]
}